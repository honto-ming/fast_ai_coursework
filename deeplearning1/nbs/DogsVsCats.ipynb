{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dogs vs. Cats - Image Recognition from (almost) Scratch in 1 hour\n",
    "For this example we will be looking at implementing VGG 16-layer CNN that won the ImageNet competion in 2016, and then finetuning it to recognize dog pictures vs cat pictures.\n",
    "\n",
    "The Cats vs. Dogs probalem is actually an old Captcha challenge and is available on Kaggle at https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition\n",
    "\n",
    "The following code is heavily based on the Fast.ai course found here: http://course.fast.ai/lessons/lesson1.html\n",
    "\n",
    "Before we get started, we assume that we have downloaded the dogs vs. cats competition data from Kaggle, and have unzipped them into train/ and test/ subdirectories in the dogscatsredux/ directory in DATA_HOME_PATH. Basically all we need to do after this is to set up a cats vs. dogs image recognition CNN is the following:\n",
    "\n",
    "1. Create the directory structures that Keras is expecting the photos to be in:\n",
    "  * valid/ directory\n",
    "  * cats/ and dogs/ subdirectories in train/, test/, and /valid\n",
    "  * sample/ directory\n",
    "  * train/ and valid/ subdirectories in sample/\n",
    "  * cats/ and dogs/ subdirectories in sample/train/ and sample/valid/\n",
    "2. Copy & move pictures to the above directories \n",
    "  * Move a random 10% of pictures from train/ to valid/\n",
    "  * Copy 200 random pictures from train/ to sample/train\n",
    "  * Copy 20 random pictures from valid/ to sample/valid\n",
    "  * For each of train/, valid/, sample/train, and sample/test, move all cat pics into cats/ and dog pics into dogs/  \n",
    "  * For test/, move all pics into unknown/\n",
    "3. Fine-tune the vgg model\n",
    "  * load the existing model architecture and the pre-trained weights\n",
    "  * remove the last dense softmax layer\n",
    "  * replace with 2 class dense softmax layer \n",
    "  * retrain last layer's weights\n",
    "4. Generate predictions\n",
    "5. Validate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from __future__ import division,print_function\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "import scipy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Deep-learning related\n",
    "from vgg16 import Vgg16\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from keras.utils.data_utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constants for directory paths\n",
    "# assumes they already exist\n",
    "NBS_HOME_PATH = '/home/honto/Online_Courses/fast_ai/fast_ai_coursework/deeplearning1/nbs'\n",
    "DATA_HOME_PATH = '/home/honto/Online_Courses/fast_ai/fast_ai_coursework/deeplearning1/data/dogscatsredux'\n",
    "MODEL_RESULTS_PATH = '/home/honto/Online_Courses/fast_ai/fast_ai_coursework/deeplearning1/results/dogscatsredux'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Create the directory structures that Keras is expecting the photos to be in:\n",
    "* valid/ directory\n",
    "* cats/ and dogs/ subdirectories in train/, test/, and /valid\n",
    "* sample/ directory\n",
    "* train/ and valid/ subdirectories in sample/\n",
    "* cats/ and dogs/ subdirectories in sample/train/ and sample/valid/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%mkdir $DATA_HOME_PATH/train/cats\n",
    "%mkdir $DATA_HOME_PATH/train/dogs\n",
    "%mkdir $DATA_HOME_PATH/valid\n",
    "%mkdir $DATA_HOME_PATH/valid/cats\n",
    "%mkdir $DATA_HOME_PATH/valid/dogs\n",
    "%mkdir $DATA_HOME_PATH/test/unknown\n",
    "%mkdir $DATA_HOME_PATH/sample\n",
    "%mkdir $DATA_HOME_PATH/sample/train\n",
    "%mkdir $DATA_HOME_PATH/sample/train/cats\n",
    "%mkdir $DATA_HOME_PATH/sample/train/dogs\n",
    "%mkdir $DATA_HOME_PATH/sample/valid\n",
    "%mkdir $DATA_HOME_PATH/sample/valid/cats\n",
    "%mkdir $DATA_HOME_PATH/sample/valid/dogs\n",
    "%mkdir $MODEL_RESULTS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Copy & move pictures to the above directories \n",
    "* Move a random 10% of pictures from train/ to valid/\n",
    "* Copy 200 random pictures from train/ to sample/\n",
    "* For each of train/, valid/, sample/train, and sample/test, move all cat pics into cats/ and dog pics into dogs/ \n",
    "* For test/, move all pics into unknown/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%cd $DATA_HOME_PATH/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# move 2500 from train/ to valid/\n",
    "# get all the pic filenames (['cat.8033.jpg', 'cat.6541.jpg', ...])\n",
    "g = glob('*.jpg')\n",
    "# randomly permute g and get the first  \n",
    "valid_filenames = np.random.permutation(g)\n",
    "for i in range(2500):\n",
    "    os.rename(valid_filenames[i], DATA_HOME_PATH + '/valid/' + valid_filenames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy 200 random pictures from train/ to sample/train\n",
    "g = glob('*.jpg')\n",
    "# randomly permute g and get the first  \n",
    "valid_filenames = np.random.permutation(g)\n",
    "for i in range(200):\n",
    "   copyfile(valid_filenames[i], DATA_HOME_PATH + '/sample/train/' + valid_filenames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Copy 20 random pictures from valid/ to sample/train\n",
    "%cd $DATA_HOME_PATH/dogscatsredux/valid\n",
    "g = glob('*.jpg')\n",
    "# randomly permute g and get the first  \n",
    "valid_filenames = np.random.permutation(g)\n",
    "for i in range(20):\n",
    "   copyfile(valid_filenames[i], DATA_HOME_PATH + '/sample/valid/' + valid_filenames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each of train/, valid/, sample/train, and sample/test, move all cat pics into cats/ and dog pics into dogs/  \n",
    "%cd $DATA_HOME_PATH/train\n",
    "%mv cat.*.jpg cats/\n",
    "%mv dog.*.jpg dogs/\n",
    "\n",
    "%cd $DATA_HOME_PATH/valid\n",
    "%mv cat.*.jpg cats/\n",
    "%mv dog.*.jpg dogs/\n",
    "\n",
    "%cd $DATA_HOME_PATH/sample/train\n",
    "%mv cat.*.jpg cats/\n",
    "%mv dog.*.jpg dogs/\n",
    "\n",
    "%cd $DATA_HOME_PATH/sample/valid\n",
    "%mv cat.*.jpg cats/\n",
    "%mv dog.*.jpg dogs/\n",
    "\n",
    "%cd $NBS_HOME_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For test/, move all pics into unknown/\n",
    "%cd $DATA_HOME_PATH/test\n",
    "%mv *.jpg unknown/\n",
    "%cd $NBS_HOME_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Fine-tune the vgg model\n",
    "* load the existing model architecture and weights\n",
    "* remove the last dense softmax layer\n",
    "* replace with 2 class dense softmax layer \n",
    "* retrain last layer's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NO_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper functions to build the VGG 16 model\n",
    "\n",
    "# VGG architecture expects to have the layers in bgr order when most pictures are in rgb\n",
    "# Also, VGG is optimized when levels are 0-meaned to the Imagenet training set RGB levels\n",
    "vgg_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape((3,1,1))\n",
    "def vgg_preprocess(x):\n",
    "    x = x - vgg_mean\n",
    "    return x[:, ::-1] # reverse axis rgb->bgr\n",
    "\n",
    "# to build the model \n",
    "def Vgg16():\n",
    "    FILE_PATH = 'http://www.platform.ai/models/'\n",
    "    \n",
    "    model = Sequential()\n",
    "    # 0-mean\n",
    "    model.add(Lambda(vgg_preprocess, input_shape=(3,224,224), output_shape=(3,224,224)))\n",
    "    \n",
    "    # 2 layers of 64 3x3 filters\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    # Max-pooling to reduce image to 112x112 pixels\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    # 2 layers of 128 3x3 filters\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    # Max-pooling to reduce image to 56x56 pixels\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    # 3 layers of 256 3x3 filters\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    # Max-pooling to reduce image to 28x28 pixels\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    # 3 layers of 512 3x3 filters\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    # Max-pooling to reduce image to 14x14 pixels\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    # 3 layers of 512 3x3 filters\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    # Max-pooling to reduce image to 7x7 pixels\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    # flatten to one long 25088-D vector\n",
    "    model.add(Flatten())\n",
    "    # 2 additional fully connected layers of 4096 nodes\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    # final fully connected softmax layer to predict probabilities for each of the 1000 image classes\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "    \n",
    "    # load the pre-trained weights so we don't have to train it with the entire Imagenet dataset\n",
    "    fname = 'vgg16.h5'\n",
    "    model.load_weights(get_file(fname, FILE_PATH+fname, cache_subdir='models'))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = Vgg16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model is trained on 1000 types with multiple types of dogs and cats, we want to adapt this model to predict dogs vs. cats. To do this we will remove the last fully connect softmax layer with 1000 nodes to one with 2 nodes - one for cats and one for dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the last dense softmax layer\n",
    "model.pop()\n",
    "# set the other layer weights to NOT get trained during fine tuning\n",
    "for l in model.layers:\n",
    "    l.trainable = False\n",
    "# add the dense layer\n",
    "# input dimension is detected from the last layer in the current model\n",
    "model.add(Dense(output_dim=2, activation='softmax'))\n",
    "# let's see the layers \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to configure the learning process (optimizer, loss, metric)\n",
    "model.compile(optimizer=Adam(lr=0.001), \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the dogs and cats images we downloaded from Kaggle and \"fine-tune\" th model (i.e. re-train the last layer's 8194 weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First we get the images from train and valid in batches. \n",
    "# We have to use Keras' generator methods when doing things in batches.\n",
    "img_gen = image.ImageDataGenerator()\n",
    "# read from directory\n",
    "# Class labels are inferred from sub-directory structure\n",
    "train_batches = img_gen.flow_from_directory(DATA_HOME_PATH + '/train/', \n",
    "                                            target_size=(224,224),\n",
    "                                            class_mode='categorical', \n",
    "                                            # categorical returns 2-D OHE truth labels\n",
    "                                            batch_size = BATCH_SIZE,\n",
    "                                            shuffle=True\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now we fit the model\n",
    "model.fit_generator(train_batches, \n",
    "                    samples_per_epoch=train_batches.nb_sample, \n",
    "                    nb_epoch=NO_EPOCHS, \n",
    "                    verbose=1, \n",
    "                    validation_data=val_batches, \n",
    "                    nb_val_samples=val_batches.nb_sample\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remind us what our loss functions and performance metric is\n",
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is basically the % of correct predictions.\n",
    "Categorical cross-entropy can be thought of as a more nuanced accuracy measure where it takes into account the confidence of each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's save the weights so we don't have to re-train in the future\n",
    "# model.save_weights(MODEL_RESULTS_PATH + '/finetune1.h5')\n",
    "model.load_weights(MODEL_RESULTS_PATH + '/finetune1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Generate predictions\n",
    "Now let us generate predictions with the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# USe the ImageGenerator to read in images\n",
    "val_batches = img_gen.flow_from_directory(DATA_HOME_PATH + '/valid/', \n",
    "                                          target_size=(224,224),\n",
    "                                          class_mode='categorical',\n",
    "                                          batch_size = 1,\n",
    "                                          shuffle=False\n",
    "                                         )\n",
    "# generate predictions (20 x 2 array of probs)\n",
    "val_pred = model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "val_pred_probs = val_pred[:,0]\n",
    "# what about the labels/classes?\n",
    "val_pred_lbls = np.argmax(val_pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Validate Predictions\n",
    "To validate predictions, we will sample and examine the following:\n",
    "\n",
    "1. A few correct labels at random\n",
    "2. A few incorrect labels at random\n",
    "3. The most correct labels of each class (ie those with highest probability that are correct)\n",
    "4. The most incorrect labels of each class (ie those with highest probability that are incorrect)\n",
    "5. The most uncertain labels (ie those with probability closest to 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first, let's get the filenames so we can view the pictures\n",
    "filenames = val_batches.filenames\n",
    "# Number of images to view for each visualization task\n",
    "n_view = 4\n",
    "# the truth labels\n",
    "val_lbls = val_batches.classes\n",
    "val_lbls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Plotting helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plots(ims, figsize=(12,6), rows=1, interp=False, titles=None):\n",
    "    if type(ims[0]) is np.ndarray:\n",
    "        ims = np.array(ims).astype(np.uint8)\n",
    "        if (ims.shape[-1] != 3):\n",
    "            ims = ims.transpose((0,2,3,1))\n",
    "    f = plt.figure(figsize=figsize)\n",
    "    for i in range(len(ims)):\n",
    "        sp = f.add_subplot(rows, len(ims)//rows, i+1)\n",
    "        sp.axis('Off')\n",
    "        if titles is not None:\n",
    "            sp.set_title(titles[i], fontsize=16)\n",
    "        plt.imshow(ims[i], interpolation=None if interp else 'none')\n",
    "        \n",
    "        \n",
    "def plots_idx(idx, titles=None):\n",
    "    # idx = index of the filenames we want to plot\n",
    "    plots([image.load_img(DATA_HOME_PATH + '/sample/valid/' + filenames[i]) for i in idx], titles=titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1 - correct labels\n",
    "correct_idx = np.where(val_pred_lbls==val_lbls)[0]\n",
    "# randomize\n",
    "correct_idx_sample = np.random.permutation(correct_idx)[:n_view]\n",
    "# plot \n",
    "plots_idx(correct_idx_sample, val_pred_probs[correct_idx_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 - random incorrect labels\n",
    "incorrect_idx = np.where(val_pred_lbls!=val_lbls)[0]\n",
    "# randomize\n",
    "incorrect_idx_sample = np.random.permutation(incorrect_idx)[:n_view]\n",
    "# plot \n",
    "plots_idx(incorrect_idx_sample, val_pred_probs[incorrect_idx_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 - Most correct dogs\n",
    "correct_dogs_idx = np.where((val_pred_lbls==1) & (val_pred_lbls==val_lbls))[0]\n",
    "most_correct_dogs = np.argsort(val_pred_probs[correct_dogs_idx])[:n_view]\n",
    "# plot \n",
    "plots_idx(correct_dogs_idx[most_correct_dogs], val_pred_probs[correct_dogs_idx][most_correct_dogs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3b - Most correct cats\n",
    "correct_cats_idx = np.where((val_pred_lbls==0) & (val_pred_lbls==val_lbls))[0]\n",
    "most_correct_cats = np.argsort(val_pred_probs[correct_cats_idx])[::-1][:n_view] # desc sort\n",
    "# plot \n",
    "plots_idx(correct_cats_idx[most_correct_cats], val_pred_probs[correct_cats_idx][most_correct_cats])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4 - Most Incorrect predictions. First, those that are most confident are cats but are dogs\n",
    "incorrect_dogs_idx = np.where((val_pred_lbls==0) & (val_pred_lbls!=val_lbls))[0]\n",
    "most_incorrect_dogs = np.argsort(val_pred_probs[incorrect_dogs_idx])[::-1][:n_view] # desc sort\n",
    "# plot \n",
    "plots_idx(incorrect_dogs_idx[most_incorrect_dogs], val_pred_probs[incorrect_dogs_idx][most_incorrect_dogs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4b -  Most Incorrect predictions. Those that are most confident are dogs but are cats\n",
    "incorrect_cats_idx = np.where((val_pred_lbls==1) & (val_pred_lbls!=val_lbls))[0]\n",
    "most_incorrect_cats = np.argsort(val_pred_probs[incorrect_cats_idx])[:n_view] # smallest\n",
    "# plot \n",
    "plots_idx(incorrect_cats_idx[most_incorrect_cats], val_pred_probs[incorrect_cats_idx][most_incorrect_cats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 5 - The most uncertain labels (ie those with probability closest to 0.5)\n",
    "uncertainty = np.abs(0.5-val_pred_probs)\n",
    "most_uncertain = np.argsort(uncertainty)[:n_view] # argsort returns an index array\n",
    "# plot \n",
    "plots_idx(most_uncertain, val_pred_probs[most_uncertain])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
